{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Including required python libraries used in this project\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "import emoji\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Input, Dropout, SimpleRNN,LSTM, Activation\n",
    "from keras.utils import np_utils\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reading data for training and testing\n",
    "train = pd.read_csv('train_emoji.csv',header=None)\n",
    "test = pd.read_csv('test_emoji.csv',header=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>never talk to me again</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>I am proud of your achievements</td>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>It is the worst day in my life</td>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Miss you so much</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>[0]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>food is life</td>\n",
       "      <td>4</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 0  1   2     3\n",
       "0           never talk to me again  3 NaN   NaN\n",
       "1  I am proud of your achievements  2 NaN   NaN\n",
       "2   It is the worst day in my life  3 NaN   NaN\n",
       "3                 Miss you so much  0 NaN   [0]\n",
       "4                     food is life  4 NaN   NaN"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking data by showing first 5 rows of the train data\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>I want to eat\\t</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>he did not answer\\t</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>he got a raise\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>she got me a present\\t</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ha ha ha it was so funny\\t</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            0  1\n",
       "0             I want to eat\\t  4\n",
       "1         he did not answer\\t  3\n",
       "2            he got a raise\\t  2\n",
       "3      she got me a present\\t  0\n",
       "4  ha ha ha it was so funny\\t  2"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking data by showing first 5 rows of the test data\n",
    "test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating dictionary for some emoji's, consisting of key - number and value - emoji \n",
    "emoji_dict = { 0 : \":heart:\", 1 : \":baseball:\", 2 : \":smile:\", 3 : \":disappointed:\", 4 : \":fork_and_knife:\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 â¤\n",
      "1 âš¾\n",
      "2 ðŸ˜„\n",
      "3 ðŸ˜ž\n",
      "4 ðŸ´\n"
     ]
    }
   ],
   "source": [
    "# Printing each emoji icon by emojizing each emoji\n",
    "for index in emoji_dict.keys():\n",
    "    print (index,end=\" \")\n",
    "    print (emoji.emojize(emoji_dict[index], use_aliases=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(132,) (132,) (56,) (56,)\n",
      "-------------------------\n",
      "never talk to me again 3\n"
     ]
    }
   ],
   "source": [
    "# Creating training and testing data\n",
    "X_train = train[0]\n",
    "Y_train = train[1]\n",
    "\n",
    "X_test = test[0]\n",
    "Y_test = test[1]\n",
    "\n",
    "print (X_train.shape, Y_train.shape, X_test.shape, Y_test.shape)\n",
    "print (\"-------------------------\")\n",
    "print (X_train[0],Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting the train data from sentences to words\n",
    "for ix in range(X_train.shape[0]):\n",
    "    X_train[ix] = X_train[ix].split()\n",
    "\n",
    "# Splitting the test data from sentences to words\n",
    "for ix in range(X_test.shape[0]):\n",
    "    X_test[ix] = X_test[ix].split()\n",
    "    \n",
    "# Converting labels into categorical form\n",
    "Y_train = np_utils.to_categorical(Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['never', 'talk', 'to', 'me', 'again'] [0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "# Now checking the above conversion by printing train and test data at 0th index\n",
    "print (X_train[0],Y_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10]),\n",
       " array([ 4,  5, 26, 35, 20, 21, 11,  5,  1,  4], dtype=int64))"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check maximum length of sentence in training data\n",
    "np.unique(np.array([len(ix) for ix in X_train]) , return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2, 3, 4, 5, 6, 7, 8]),\n",
       " array([ 3, 12, 16, 17,  3,  4,  1], dtype=int64))"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# To check maximum length of senetence in testing data\n",
    "np.unique(np.array([len(ix) for ix in X_test]) , return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Creating  embeddings dictionary with key = word and value = list of words in glove vector\n",
    "embeddings_index = {}\n",
    "\n",
    "f = open('glove.6B.50d.txt', encoding=\"utf8\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50,)"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking length of a particular word\n",
    "embeddings_index[\"i\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.31093674898147583"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import spatial\n",
    "# Checking cosine similarity of words happy and sad\n",
    "spatial.distance.cosine(embeddings_index[\"happy\"], embeddings_index[\"sad\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.18572336435317993"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking cosine similarity of words India and Delhi\n",
    "spatial.distance.cosine(embeddings_index[\"india\"], embeddings_index[\"delhi\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.19746702909469604"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Checking cosine similarity of words france and paris\n",
    "spatial.distance.cosine(embeddings_index[\"france\"], embeddings_index[\"paris\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling the embedding matrix\n",
    "embedding_matrix_train = np.zeros((X_train.shape[0], 10, 50))\n",
    "embedding_matrix_test = np.zeros((X_test.shape[0], 10, 50))\n",
    "\n",
    "for ix in range(X_train.shape[0]):\n",
    "    for ij in range(len(X_train[ix])):\n",
    "        embedding_matrix_train[ix][ij] = embeddings_index[X_train[ix][ij].lower()]\n",
    "        \n",
    "for ix in range(X_test.shape[0]):\n",
    "    for ij in range(len(X_test[ix])):\n",
    "        embedding_matrix_test[ix][ij] = embeddings_index[X_test[ix][ij].lower()]        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshaping and transposing the matrices to fit into the model\n",
    "embedding_matrix_train = embedding_matrix_train.reshape(132, 500)\n",
    "embedding_matrix_test = embedding_matrix_test.reshape(56, 500)\n",
    "embedding_matrix_train = embedding_matrix_train.T\n",
    "embedding_matrix_test = embedding_matrix_test.T\n",
    "Y_train = Y_train.T\n",
    "Y_test = Y_test.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(500, 132) (500, 56) (5, 132)\n"
     ]
    }
   ],
   "source": [
    "print (embedding_matrix_train.shape, embedding_matrix_test.shape, Y_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a basic model (EBPA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "n_layers = 3\n",
    "layers = [500, 256, 100 , 50 , 5] #List containing number of  neurons in each layer\n",
    "assert(layers[0] == embedding_matrix_train.shape[0])  #first layer = input layer\n",
    "assert(layers[-1] == Y_train.shape[0]) #number of outputs should be equal to number of classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#softmax activation will be used to get number in last layer\n",
    "#We are using ReLu activation elsewhere\n",
    "def softmax(z):\n",
    "    z = np.exp(z)\n",
    "    z = z/np.sum(z , axis = 0 , keepdims = True , dtype=np.float32) #sum along a column\n",
    "    return z\n",
    "\n",
    "def relu(z):\n",
    "    temp = z > 0\n",
    "    z = z * temp\n",
    "    return z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def d_relu(A):\n",
    "    return A >= 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "#defining network variables\n",
    "a = {}  #activations\n",
    "w = {}  #Weights\n",
    "b = {}  #bias\n",
    "# del_a = None   #d(error)/d(activation)\n",
    "# del_w = None   #d(error)/d(weight)\n",
    "# del_b = None   #d(error)/d(bias)\n",
    "a[0] =  embedding_matrix_train #input layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_nn(layers):\n",
    "    for i in range(1 , len(layers)):\n",
    "        w[i] = np.random.randn(layers[i] , layers[i - 1])/100. \n",
    "        b[i] = np.zeros((layers[i] , 1))\n",
    "        a[i] = relu(np.dot(w[i] , a[i - 1]) + b[i]) #dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains neural network for single epoch\n",
    "#It implements Batch Gradient Descent instead of Stochastic Gradient Descent\n",
    "def train_one_epoch(alpha):\n",
    "    #Forward propagation:\n",
    "    \n",
    "    #using relu for all layers except last\n",
    "    for i  in range(1 , len(layers) - 1):\n",
    "        a[i] = relu(np.dot(w[i] , a[i - 1]) + b[i])\n",
    "        \n",
    "    last_index = len(layers) - 1\n",
    "    \n",
    "    #using softmax for last layer\n",
    "    a[last_index] = softmax(np.dot(w[last_index] , a[last_index - 1]) + b[last_index])\n",
    "    output = a[last_index]\n",
    "    \n",
    "    \n",
    "    #Error Calculation:\n",
    "    #softmax crossentropy was used \n",
    "    m = 132\n",
    "    error = -1*(Y_train*np.log(output))\n",
    "    error = 1/m*np.sum(np.sum(error , axis = 1 , keepdims = True) )\n",
    "    \n",
    "     \n",
    "    #Back propagation:\n",
    "    \n",
    "    #for last layer (with softmax activation)\n",
    "    del_a =  -1 * np.divide(Y_train , output)\n",
    "    del_z = a[last_index] - Y_train      #z represents logits, activation(z) = a\n",
    "    del_w = 1/m *np.dot(del_z , a[last_index - 1].T)\n",
    "    del_b = 1/m * np.sum(del_z , axis = 1 , keepdims=True)\n",
    "    del_a = np.dot(w[last_index].T , del_z)\n",
    "    \n",
    "    #weight updation:\n",
    "    w[last_index] -= del_w*alpha\n",
    "    b[last_index] -= del_b*alpha\n",
    "    \n",
    "    #for all layers except last\n",
    "    for i in range(last_index - 1, 0, -1):\n",
    "        del_z = del_a * d_relu(a[i])\n",
    "        del_w = 1/m *np.dot(del_z , a[i - 1].T)\n",
    "        del_b = 1/m*np.sum(del_z , axis = 1 , keepdims=True)\n",
    "        del_a = np.dot(w[i].T , del_z)\n",
    "        \n",
    "        #weight updation:\n",
    "        w[i] -= del_w*alpha\n",
    "        b[i] -= del_b*alpha\n",
    "        \n",
    "    return error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "#trains NN for n_epochs epochs\n",
    "def train(n_epochs , learning_rate = 0.01):\n",
    "    for i in range(n_epochs):\n",
    "        print(\"Epoch :\" + str(i + 1) , end=\" ...... \")\n",
    "        error = train_one_epoch(learning_rate)\n",
    "        print(\"\\t Error = \" + str(error))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "#forward propagates a case which we want to test\n",
    "def find_output(sample_case):\n",
    "    last_index = len(layers) - 1\n",
    "    a[0] = sample_case\n",
    "    for i in range(1 , len(layers) - 1):\n",
    "        a[i] = relu(np.dot(w[i] , a[i - 1]) + b[i])\n",
    "    return softmax(np.dot(w[last_index] , a[last_index - 1]) + b[last_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "initialize_nn(layers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch :1 ...... \t Error = 1.6094340981260142\n",
      "Epoch :2 ...... \t Error = 1.5891193229169924\n",
      "Epoch :3 ...... \t Error = 1.5762703547913957\n",
      "Epoch :4 ...... \t Error = 1.5681846368100518\n",
      "Epoch :5 ...... \t Error = 1.5631058948750505\n",
      "Epoch :6 ...... \t Error = 1.559912262475861\n",
      "Epoch :7 ...... \t Error = 1.5578981518641875\n",
      "Epoch :8 ...... \t Error = 1.5566221143374668\n",
      "Epoch :9 ...... \t Error = 1.5558091450627223\n",
      "Epoch :10 ...... \t Error = 1.555287312442391\n",
      "Epoch :11 ...... \t Error = 1.554949151830833\n",
      "Epoch :12 ...... \t Error = 1.5547275776949658\n",
      "Epoch :13 ...... \t Error = 1.5545800268681167\n",
      "Epoch :14 ...... \t Error = 1.554479470588075\n",
      "Epoch :15 ...... \t Error = 1.5544090293815906\n",
      "Epoch :16 ...... \t Error = 1.5543581826691606\n",
      "Epoch :17 ...... \t Error = 1.5543200092331362\n",
      "Epoch :18 ...... \t Error = 1.55428976252486\n",
      "Epoch :19 ...... \t Error = 1.5542645888882893\n",
      "Epoch :20 ...... \t Error = 1.554242531864993\n",
      "Epoch :21 ...... \t Error = 1.5542224733733447\n",
      "Epoch :22 ...... \t Error = 1.5542033710718133\n",
      "Epoch :23 ...... \t Error = 1.5541847609347268\n",
      "Epoch :24 ...... \t Error = 1.5541662881923681\n",
      "Epoch :25 ...... \t Error = 1.5541477516430322\n",
      "Epoch :26 ...... \t Error = 1.5541287606968428\n",
      "Epoch :27 ...... \t Error = 1.5541088778050889\n",
      "Epoch :28 ...... \t Error = 1.5540880357536548\n",
      "Epoch :29 ...... \t Error = 1.554066094902797\n",
      "Epoch :30 ...... \t Error = 1.554042910047646\n",
      "Epoch :31 ...... \t Error = 1.5540183081918364\n",
      "Epoch :32 ...... \t Error = 1.5539920127827294\n",
      "Epoch :33 ...... \t Error = 1.5539639785335586\n",
      "Epoch :34 ...... \t Error = 1.5539337693207507\n",
      "Epoch :35 ...... \t Error = 1.5539011947992925\n",
      "Epoch :36 ...... \t Error = 1.5538657405548548\n",
      "Epoch :37 ...... \t Error = 1.5538273524936912\n",
      "Epoch :38 ...... \t Error = 1.553785281411111\n",
      "Epoch :39 ...... \t Error = 1.5537390968450628\n",
      "Epoch :40 ...... \t Error = 1.5536885739769948\n",
      "Epoch :41 ...... \t Error = 1.5536332442535588\n",
      "Epoch :42 ...... \t Error = 1.5535721542909589\n",
      "Epoch :43 ...... \t Error = 1.5535040216000653\n",
      "Epoch :44 ...... \t Error = 1.5534278622363724\n",
      "Epoch :45 ...... \t Error = 1.5533424842949664\n",
      "Epoch :46 ...... \t Error = 1.5532457215605473\n",
      "Epoch :47 ...... \t Error = 1.553136424808596\n",
      "Epoch :48 ...... \t Error = 1.5530127255072241\n",
      "Epoch :49 ...... \t Error = 1.5528701302406025\n",
      "Epoch :50 ...... \t Error = 1.5527058057200795\n"
     ]
    }
   ],
   "source": [
    "train(50 , 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = find_output(embedding_matrix_test)\n",
    "# print(pred.shape)\n",
    "pred = np.argmax(pred, axis=0)\n",
    "# print(pred.shape, Y_test.shape, Y_train.shape, embedding_matrix_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.32142857142857145"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "float(sum(pred==Y_test))/embedding_matrix_test.shape[1]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
